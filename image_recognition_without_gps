from dronekit import *
from pymavlink import mavutil
import time
from math import *
import cv2
import numpy as np
import imutils.video

def detection():
  print('Starting detection')

  # Initialising stuff
  counter = 0
  marker = 1
  # start = time.time()
  # end = time.time()

  cap = cv2.VideoCapture(0)

  time.sleep(0.1)  # allows the camera to start-up
  print('Camera on')
  # Run detection script while still going through waypoints
  while True:
    if counter == 0 or start - end < 5:
      _, frame = cap.read()
      start = time.time()

      gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # converts to gray
      blurred = cv2.GaussianBlur(gray, (5, 5), 0)  # blur the gray image for better edge detection
      edged = cv2.Canny(blurred, 50, 10)  # the lower the value the more detailed it would be

      # find contours in the thresholded image and initialize the
      (contours, _) = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)  # grabs contours

      # shape detector
      for c in contours:
        peri = cv2.arcLength(c, True)  # grabs the contours of each points to complete a shape
        # get the approx. points of the actual edges of the corners
        approx = cv2.approxPolyDP(c, 0.01 * peri, True)
        if 4 <= len(approx) <= 6:
          (x, y, w, h) = cv2.boundingRect(approx)  # gets the (x,y) of the top left of the square and the (w,h)
          aspectRatio = w / float(h)  # gets the aspect ratio of the width to height
          area = cv2.contourArea(c)  # grabs the area of the completed square
          hullArea = cv2.contourArea(cv2.convexHull(c))
          solidity = area / float(hullArea)
          keepDims = w > 25 and h > 25
          keepSolidity = solidity > 0.9  # to check if it's near to be an area of a square
          keepAspectRatio = 0.8 <= aspectRatio <= 1.2
          if keepDims and keepSolidity and keepAspectRatio:  # checks if the values are true
            # captures the regoin of interest with a 5 pixel wider in all 2D directions
            roi = frame[y - 5:y + h + 5, x - 5:x + w + 5]

            # The image will go through the same procesdure to detect the four corners and rotate the picture
            grayn = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
            blurredn = cv2.GaussianBlur(grayn, (5, 5), 0)
            edgedn = cv2.Canny(blurredn, 50, 100)
            (contours, _) = cv2.findContours(edgedn.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

            for cn in contours:
              perin = cv2.arcLength(cn, True)
              approxn = cv2.approxPolyDP(cn, 0.01 * perin, True)
              if 4 <= len(approxn) <= 6:
                (x, y, w, h) = cv2.boundingRect(approxn)
                aspectRatio = w / float(h)
                keepAspectRatio = 0.8 <= aspectRatio <= 1.2
                if keepAspectRatio:
                  angle = cv2.minAreaRect(approxn)[-1]  # -1 is the angle the rectangle is at

                  if angle == 0:
                    angle_n = angle
                  if -45 > angle:
                    angle_n = -(90 + angle)
                  else:
                    angle_n = -angle

                  rotated = imutils.rotate_bound(roi, angle_n)

                  # Convert the image to grayscale and turn to outline of  the letter
                  g_rotated = cv2.cvtColor(rotated, cv2.COLOR_BGR2GRAY)
                  b_rotated = cv2.GaussianBlur(g_rotated, (5, 5), 0)
                  t_rotated = cv2.adaptiveThreshold(b_rotated, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,
                                                    11,
                                                    0)
                  kernel = np.ones((4, 4), np.uint8)
                  ee = cv2.morphologyEx(t_rotated, cv2.MORPH_OPEN, kernel)
                  e_rotated = cv2.Canny(b_rotated, 50, 100)

                  # uses the outline to detect the corners for the cropping of the image
                  (contours, _) = cv2.findContours(e_rotated.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

                  for cny in contours:
                    periny = cv2.arcLength(cny, True)
                    approxny = cv2.approxPolyDP(cny, 0.01 * perin, True)
                    if 4 <= len(approxny) <= 6:
                      (xx, yy), (ww, hh), angle = cv2.minAreaRect(approxny)
                      aspectRatio = ww / float(hh)
                      keepAspectRatio = 0.8 <= aspectRatio <= 1.2
                      keep_angle = angle == 0, 90, 180, 270, 360
                      if keepAspectRatio and keep_angle:
                        (xxx, yyy, www, hhh) = cv2.boundingRect(approxny)
                        nnc_roi = ee[yyy:yyy + hhh, xxx:xxx + www]

                        # time that the target has been last seen
                        end = time.time()

                        # keep count of number of saved images
                        counter = counter + 1
                        cv2.imwrite("rotated%d.png" % counter, nnc_roi)
                        print("Detected and saved a target")
    else:
      print("starting recognition")
      recognition(counter, marker)
      counter = 0
      print("detection of marker %s located", marker)
      marker = marker + 1


    cv2.imshow('frame', frame)

    k = cv2.waitKey(5) & 0xFF
    if k == 27:
      break

  cap.release()
  cv2.destroyAllWindows()


def recognition(counter, marker):
  print('Starting recognition thread')

  guesses = [0] * 35  # create a list of 35 lists
  for i in range(counter):
    img = cv2.imread("rotated%d.png" % (i + 1))
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    # set heights and width to be able to read the image when comparing to flatten images
    h = 30
    w = 20

    resize = cv2.resize(gray, (w, h))  # resizes the images
    nparesize = resize.reshape(1, w * h).astype(np.float32)  # changes into 1D array

    knn = cv2.ml.KNearest_create()  # initalise the knn
    # joins the train data with the train_labels
    knn.train(npaFlattenedImages, cv2.ml.ROW_SAMPLE, npaClassifications)
    # looks for the 3 nearest neighbours comparing to the flatten images (k = neighbours)
    retval, npaResults, neigh_resp, dists = knn.findNearest(nparesize, k=3)

    # current guess
    gg = int(npaResults[0][0])
    # Tranform guess in ASCII format into range 0-35
    if 48 <= gg <= 57:
      guesses[gg - 48] += 1
    elif 65 <= gg <= 90:
      guesses[gg - 57] += 1

  # find modal character guess
  # Initialise mode and prev variables for first loop through
  mode = 0
  prev = guesses[0]
  for j in range(35):
    new = guesses[j]
    if new > prev:
      prev = guesses[j]
      mode = j
  # Transform back into ASCII
  if 0 <= mode <= 8:
    mode = mode + 48
  elif 9 <= mode <= 34:
    mode = mode + 57

  print(chr(mode) + " is located for marker %s", marker)

  return


# Load training and classification data
npaClassifications = np.loadtxt("classifications.txt", np.float32)
npaFlattenedImages = np.loadtxt("flattened_images.txt", np.float32)

# reshape classifications array to 1D for k-nn
npaClassifications = npaClassifications.reshape((npaClassifications.size, 1))

detection()
